{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3v5HIUdDvY5"
   },
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 3\n",
    "\n",
    "**Warning 1**: some problems require (especially the lemmatization part) significant amount of time, so **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.6.2)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.5.1\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.5.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.1\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.23.4)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=197a6bab545a9bc1dd9d4747322c06d131a25bccc7d167417d3c11a62d0f1c6b\n",
      "  Stored in directory: /home/vodavydov/.cache/pip/wheels/9b/13/01/6f3a7fd641f90e1f6c8c7cded057f3394f451f340371c68f3d\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.1.3 scipy-1.9.3 sklearn-0.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F7t9dYtdDvZC"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIHtwV6vDvZD"
   },
   "source": [
    "## PART 1: Logit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7XKEWcVDvZD"
   },
   "source": [
    "We consider a binary classification problem. For prediction, we would like to use a logistic regression model. For regularization we add a combination of the $l_2$ and $l_1$ penalties (Elastic Net). \n",
    "\n",
    "Each object in the training dataset is indexed with $i$ and described by pair: features $x_i\\in\\mathbb{R}^{K}$ and binary labels $y_i$. The model parametrized with bias $w_0\\in\\mathbb{R}$ and weights $w\\in\\mathbb{R}^K$.\n",
    "\n",
    "The optimization problem with respect to the $w_0, w$ is the following (Elastic Net Loss):\n",
    "\n",
    "$$L(w, w_0) = \\frac{1}{N} \\sum_{i=1}^N \\ln(1+\\exp(-y_i(w^\\top x_i+w_0))) + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1eSuDKXFVZu"
   },
   "source": [
    "#### 1. [0.5 points]  Find the gradient of the Elastic Net loss and write its formulas (better in latex format) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zjH-YnPDvZD"
   },
   "source": [
    "Поскольку взятие производной это линейная операция, то все просто\n",
    "\n",
    "$\\frac{\\delta L}{\\delta w} = -\\frac{1}{N}\\sum_{i=1}^{N}\\frac{x_iy_i}{1 + \\exp{(y_i(w^Tx_i + w_0))}} + \\gamma \\frac{w}{|w|} + 2 \\beta w$\n",
    "\n",
    "$\\frac{\\delta L}{\\delta w_0} = -\\frac{1}{N}\\sum_{i=1}^{N}\\frac{y_i}{1 + \\exp{(y_i(w^Tx_i + w_0))}}$\n",
    "\n",
    "А можно мне пожалуйста номерок человека, который этот лосс придумал. Надо пообщаться бы, вопросики есть. Например о том, что будет при y_i равном нулю? Log(2)???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_lIccN_DvZE"
   },
   "source": [
    "#### 2. [0.25 points] Implement the Elastic Net loss (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "9QNfCtV5DvZE"
   },
   "outputs": [],
   "source": [
    "def loss(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> float:\n",
    "    \n",
    "    return np.mean(np.log(1 + np.exp(-np.dot(y, np.dot(X, w) + w0))), axis=1) + gamma * np.sum(np.abs(w)) + beta * np.sum(np.square(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIVoC6UmDvZE"
   },
   "source": [
    "#### 3. [0.25 points] Implement the gradient (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "HWqBLGRADvZE"
   },
   "outputs": [],
   "source": [
    "def sigmoid(h):\n",
    "    return 1. / (1 + np.exp(-h))\n",
    "\n",
    "def get_grad(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "    \n",
    "    N = len(y)\n",
    "    \n",
    "    grad_w = -np.mean((X.T * y) / (1 + np.exp((y * np.dot(X, w) + w0))), axis=1) + gamma * w/np.abs(w) + beta * 2 * w\n",
    "    \n",
    "    grad_w0 = -np.mean(y / (1 + np.exp((y * np.dot(X, w) + w0))))\n",
    "        \n",
    "    return grad_w, grad_w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhOb8HrtDvZF"
   },
   "source": [
    "#### Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "3FxXTocHDvZF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w = [-2.73262181 -1.87176392  1.30051023  2.53598816 -2.71198278]\n",
      "grad_w0 = -0.20782319347690742\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
    "\n",
    "y = np.random.binomial(1, 0.42, size=10)\n",
    "\n",
    "w, w0 = np.random.normal(size=5), np.random.normal()\n",
    "\n",
    "grad_w, grad_w0 = get_grad(X, y, w, w0)\n",
    "print(f\"grad_w = {grad_w}\")\n",
    "print(f\"grad_w0 = {grad_w0}\")\n",
    "\n",
    "assert(np.allclose(grad_w,\n",
    "                   [-2.73262076, -1.87176281, 1.30051144, 2.53598941, -2.71198109],\n",
    "                   rtol=1e-2) & \\\n",
    "       np.allclose(grad_w0,\n",
    "                   -0.2078231418067844, \n",
    "                   rtol=1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbqLfcrRDvZF"
   },
   "source": [
    "####  4. [1 point]  Implement gradient descent which works for both tol level and max_iter stop criteria and plot the decision boundary of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIgiwQkjDvZF"
   },
   "source": [
    "The template provides basic sklearn API class. You are free to modify it in any convenient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "Thyeux0KDvZG"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "id": "erVmNR0PDvZG"
   },
   "outputs": [],
   "source": [
    "class Logit(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, beta=1.0, gamma=1.0, lr=1e-3, tolerance=0.01, max_iter=1000, random_state=42):  \n",
    "        self.beta = beta        \n",
    "        self.gamma = gamma\n",
    "        self.tolerance= tolerance\n",
    "        self.max_iter= max_iter\n",
    "        self.learning_rate = lr\n",
    "        self.random_state = random_state\n",
    "        self.w = None\n",
    "        self.w0 = None\n",
    "        # you may additional properties if you wish\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # add weights and bias and optimize Elastic Net loss over (X,y) dataset\n",
    "        # save history of optimization steps\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        n, k = X.shape\n",
    "        \n",
    "        if self.w is None:\n",
    "            self.w = np.random.normal(size=k)\n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.random.normal()\n",
    "                \n",
    "        losses = []\n",
    "        \n",
    "        for iter_num in range(self.max_iter):\n",
    "            # z = sigmoid(logit(X_train, self.w))\n",
    "            # grad = np.dot(X_train.T, (z - y)) / len(y)\n",
    "            grad_w, grad_w0 = get_grad(X, y, self.w, self.w0, self.gamma, self.beta)\n",
    "            if np.mean(grad_w) <= self.tolerance:\n",
    "                return losses\n",
    "\n",
    "            self.w -= grad_w * lr\n",
    "            self.w0 -= grad_w0 * lr\n",
    "\n",
    "            losses.append(loss(X, y, self.w, self.w0, self.gamma, self.beta))\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # return vector of predicted labels for each object from X\n",
    "        # your code here\n",
    "        pred = self.predict_proba(X)\n",
    "        print(pred)\n",
    "        return pred[0] > 0.5 or pred[1] < 0.5\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return np.array([1 / (1 + np.exp(np.dot(X, self.w) + self.w0)),\\\n",
    "                         1 / (1 + np.exp(-np.dot(X, self.w) - self.w0))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "id": "7SJX8Y6EDvZG"
   },
   "outputs": [],
   "source": [
    "# sample data to test your model\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=180, n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=42, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "id": "u41kzwGTDvZH"
   },
   "outputs": [],
   "source": [
    "# a function to plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    X1min, X2min = X.min(axis=0)\n",
    "    X1max, X2max = X.max(axis=0)\n",
    "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 200),\n",
    "                         np.linspace(X2min, X2max, 200))\n",
    "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    ypred = ypred.reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "id": "mNuYbsAoDvZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0034839  0.00348878 0.00349367 ... 0.96959023 0.96963168 0.96967307]\n",
      " [0.9965161  0.99651122 0.99650633 ... 0.03040977 0.03036832 0.03032693]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [370], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y[y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplot_decision_boundary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [347], line 8\u001b[0m, in \u001b[0;36mplot_decision_boundary\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m X1max, X2max \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m x1, x2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(np\u001b[38;5;241m.\u001b[39mlinspace(X1min, X1max, \u001b[38;5;241m200\u001b[39m),\n\u001b[1;32m      7\u001b[0m                      np\u001b[38;5;241m.\u001b[39mlinspace(X2min, X2max, \u001b[38;5;241m200\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m ypred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ypred \u001b[38;5;241m=\u001b[39m ypred\u001b[38;5;241m.\u001b[39mreshape(x1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mcontourf(x1, x2, ypred, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.4\u001b[39m)\n",
      "Cell \u001b[0;32mIn [369], line 47\u001b[0m, in \u001b[0;36mLogit.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m pred[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Logit(0,0)\n",
    "y[y == 0] = -1\n",
    "model.fit(X, y)\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi4WRhcADvZI"
   },
   "source": [
    "#### 5. [0.25 points] Plot loss diagram for the model, i.e. show the dependence of the loss function from the gradient descent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyjMDAKuDvZI"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FhSCAv_DvZJ"
   },
   "source": [
    "## PART 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYyGsSxEDvZJ"
   },
   "source": [
    "#### 6. [2 point] Using the same dataset, train SVM Classifier from Sklearn.\n",
    "Investigate how different parameters influence the quality of the solution:\n",
    "+ Try several kernels: Linear, Polynomial, RBF (and others if you wish). Some Kernels have hypermeters: don't forget to try different.\n",
    "+ Regularization coefficient \n",
    "\n",
    "Show how these parameters affect accuracy, roc_auc and f1 score. \n",
    "Make plots for the dependencies between metrics and parameters. \n",
    "Try to formulate conclusions from the observations. How sensitive are kernels to hyperparameters? How sensitive is a solution to the regularization? Which kernel is prone to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nicu_O3IDvZK"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY8q6JdCDvZK"
   },
   "source": [
    "## PART 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD4xKhYfDvZK"
   },
   "source": [
    "#### 7. [1.75 point] Form the dataset\n",
    "\n",
    "We are going to form a dataset that we will use in the following tasks for binary and multiclass classification\n",
    "\n",
    "0. Choose **six** authors that you like (specify who you've chosen) and download the <a href=\"https://www.kaggle.com/d0rj3228/russian-literature?select=prose\">relevant data</a> from **prose** section\n",
    "1. Build your own dataset for these authors: \n",
    "    * divide each text into sentences such that we will have two columns: *sentence* and *target author*, each row will contain one sentence and one target\n",
    "    * drop sentences where N symbols in a sentence < 15\n",
    "    * fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
    "    \n",
    "    sample data may look like:\n",
    "    \n",
    "    <center> \n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> sentence </th>\n",
    "            <th> author </th>\n",
    "        </tr> \n",
    "        <tr><td> Несколько лет тому назад в одном из своих поместий жил старинный русской барин, Кирила Петрович Троекуров. </td><td> Пушкин </td><td> \n",
    "        <tr><td> Уже более недели приезжий господин жил в городе, разъезжая по вечеринкам и обедам и таким образом проводя, как говорится, очень приятно время. </td><td> Гоголь </td><td> \n",
    "        <tr><td> ... </td><td> ... </td><td> \n",
    "        <tr><td> Я жил недорослем, гоняя голубей и играя в чехарду с дворовыми мальчишками. </td><td> Пушкин </td><td>         \n",
    "    </table>\n",
    "</center>\n",
    "     \n",
    "2. Preprocess (tokenize and clean) the dataset \n",
    "    * tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
    "    * convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
    "    * vectorize words using both **bag of words** and **tf-idf** (use sklearn)\n",
    "    * observe and describe the difference between vectorized output (what do numbers look like after transformations and what do they represent?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVStbeQ8DvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuTi3rvnDvZL"
   },
   "source": [
    "###  Binary classification\n",
    "\n",
    "#### 8. [2 point] Train model using Logistic Regression (your own) and SVC (SVM can be taken from sklearn) \n",
    "\n",
    "* choose *two* authors from the dataset that you have formed in the previous task\n",
    "* check the balance of the classes\n",
    "* divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
    "* make several plots to address the dependence between F1 score and parameters\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn) \n",
    "* make conclusions about the performance of your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZUP1HqFDvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G1kt0qbDvZL"
   },
   "source": [
    "#### 9. [1 point] Analysing ROC AUC\n",
    "\n",
    "It is possible to control the proportion of statistical errors of different types using different thresholds for choosing a class. Plot ROC curves for Logistic Regression and SVC, show the threshold on ROC curve plots. Choose such a threshold that your models have no more than 30% of false positive errors rate. Pay attention to `thresholds` parameter in sklearn roc_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ2GZ8-uDvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-qubaK4DvZM"
   },
   "source": [
    "### Multiclass logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJQone-LDvZM"
   },
   "source": [
    "#### 10. [1 point] Take the One-VS-One classifier (use sklearn) and apply to Logit model (one you've made in the 4th task) in order to get multiclass linear classifier\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\">OneVsOneClassifier</a>\n",
    "\n",
    "* use the data you got at the previous step for 6 authors\n",
    "* divide the data into train and test samples with 0.7 split rate\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score)\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute all possible and relevant metrics for test sample (use sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4lR8qJ7DvZM"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_v7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

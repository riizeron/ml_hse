{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bozobTOUbLPg"
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Seminar 9: Gradient boosting\n",
    "**Author**: Vadim Kokhtev\n",
    "\n",
    "<center> \n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> Class Teachers </th>\n",
    "            <th> Contact </th>\n",
    "            <th> Group </th>\n",
    "            <th> TA (contact) </th>\n",
    "        </tr> \n",
    "        <tr><td> Andrey Egorov </td><td> tg: @andrei_egorov </td><td> БПИ201, БПИ202 </td><td> Andrei Dyadynov (tg: @mr_dyadyunov), Nikita Tatarinov (tg: @NickyOL) </td></tr>\n",
    "        <tr><td> Kirill Bykov </td><td> tg: @darkydash </td><td> БПИ203, БПИ204 </td><td> Anastasia Egorova (tg: @wwhatisitt), Elizaveta Berdina (tg: @berdina_elis) </td></tr>\n",
    "        <tr><td> Maria Tikhonova </td><td> tg: @mashkka_t </td><td> БПИ205 </td><td> Alexander Stepin (tg: @kevicia) </td></tr>\n",
    "        <tr><td> Anastasia Voronkova </td><td> tg: @kotovasyka </td><td> БПИ206, БПИ207 </td><td> Anton Alekseev (tg: @flameglamebeatskilla), Emil Akopyan (tg: @archivarius) </td></tr>        \n",
    "    </table>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTFBsyz5hkjc"
   },
   "source": [
    "# Gradient Boosting\n",
    "Gradient boosting (GB) is a machine learning algorithm developed in the late '90s that is still very popular. It produces state-of-the-art results for many commercial (and academic) applications. Gradient boosting builds an ensemble of trees one-by-one, then the predictions of the individual trees are summed.\n",
    "\n",
    "\n",
    " In boosting, several weak learners (high bias and low variance models) are combined additively to produce an ensemble with reduced bias while maintaining the low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKpIh4SNbLPm"
   },
   "source": [
    "## Gradient boosting vs Random forest\n",
    "\n",
    "First of all, we need to solve the task:\n",
    "\n",
    "A sample of $\\ell$ objects with $d$ features is given. Give the time asymptotics of learning and making predictions for a composition $a_N(x) = \\sum_{n=0}^N b_n (x)$ over decision trees $b_n$ of depth at most $D$.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "For training, we need to fit $N$ trees, so the asymptotics will be $N * T_{tree}$, where $T_{tree}$ is the complexity of constructing a single decision tree. When constructing a single tree, consider the complexity of finding the optimal partition at a single vertex. Consider how much time is spent on building a single level in the decision tree. Denote by $\\ell^i_j$ - the number of objects from the training sample that reach the vertex $j$ at the $i$ - th level.\n",
    "\n",
    "\n",
    "We need to check $d$ of features by the $\\ell^i_j - 1$ thresholds. Calculating the partitioning quality metric takes $O (\\ell^i_j)$ time. However, you can implement the $\\textit{optimal}$ algorithm with pre-calculated statistics, which for each feature can calculate the metric for all $\\ell^i_j - 1$ thresholds in $O(\\ell^i_j)$ time. Hence, a single vertex requires $d * O (\\ell^i_j) = O (d\\ell^i_j)$ time.\n",
    "\n",
    "It is clear that $\\ell^1_1 = \\ell$, since this is the root vertex. Further, for any $i, j$, it is true that $\\ell^i_j = \\ell^{i + 1}_{2j-1} + \\ell^{i + 1}_{2j}$ by the principle of source-sink conservation (the number of objects originating from one vertex to its children is equal to the number of objects entering this vertex). It follows that in total there will be only $\\ell$ objects on one level, $\\forall i ~:~ \\sum\\limits^{2^i}_{j=1} \\ell^i_j = \\ell$. So, to build one level in the decision tree, we need only $ \\sum \\limits^{2^i}_{j=1} O (d\\ell^i_j) = O (d\\ell)$, and the levels are only $D$. Therefore, to build a single tree, you need $T_{tree} = O(Dd\\ell)$ of time. We have $N$ trees, hence the total time to build all the trees, and hence for all the training: $O(NDd\\ell)$\n",
    "\n",
    "At the stage of constructing a forecast for the object $x$, it is \"passed\" through the tree from the root to the leaves, thereby passing a path of no more than $D$ internal vertices, each of which checks the predicate in constant time. Hence, we have the asymptotics for constructing the composition prediction - $O (ND)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpdD5zCBbLPn"
   },
   "source": [
    "### Changing the number of algorithms\n",
    "\n",
    "Let's compare how boosting and bagging behave with the growth of the number of basic algorithms.\n",
    "\n",
    "In the case of bagging, all the underlying algorithms are tuned to different samples from the same distribution at $\\mathbb{X} \\times \\mathbb{Y}$. At the same time, some of them may be overfitted, but averaging allows you to weaken this effect (due to the fact that for uncorrelated algorithms, the spread of the composition is $N$ times less than the spread of individual algorithms, i.e. many trees are less likely to tune in to some atypical object compared to a single tree). If $N$ is large enough, then subsequent additions of new algorithms will no longer improve the quality of the model.\n",
    "\n",
    "In the case of boosting, each algorithm is adjusted to the errors of all the previous ones, this allows you to adjust to the original distribution more and more accurately at each step. However, with a sufficiently large $N$, this is a source of overfitting, since subsequent additions of new algorithms will continue to tune to the training sample, reducing the error on it, while reducing the generalization ability of the final composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ob4m0ADnbLPo"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "QSfQ08TlbLPp",
    "outputId": "bdda7975-c42e-45b4-9a89-faae042953c2"
   },
   "outputs": [],
   "source": [
    "X_train = np.linspace(0, 1, 100)\n",
    "X_test = np.linspace(0, 1, 1000)\n",
    "\n",
    "@np.vectorize\n",
    "def target(x):\n",
    "    return x > 0.5\n",
    "\n",
    "Y_train = target(X_train) + np.random.randn(*X_train.shape) * 0.1\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.scatter(X_train, Y_train, s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZGC8Ori4bLPr",
    "outputId": "930b876c-2e4d-4ecb-999a-030f4fb0d5d8"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\n",
    "\n",
    "reg = BaggingRegressor(DecisionTreeRegressor(max_depth=2), warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in enumerate(sizes):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvvUTmd_bLPr"
   },
   "source": [
    "You can see that from some point on, the resulting function stops changing with the growth of the number of trees.\n",
    "\n",
    "Now let's do the same for gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TNagJJ-_bLPs",
    "outputId": "2e30c4f9-6285-4879-c94d-b6da356988b0"
   },
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in enumerate(sizes):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwKDroNAbLPs"
   },
   "source": [
    "Gradient boosting quickly finds a true dependency, after which it begins adjusting to specific objects in the training sample, thus, it greatly overfits.\n",
    "\n",
    "To deal with this problem, you can choose a very simple basic algorithm or\n",
    "artificially reduce the weight of new algorithms using the step $\\eta$:\n",
    "$$a_N(x) = \\sum_{n=0}^N \\eta \\gamma_N b_n(x).$$\n",
    "\n",
    "This correction slows down learning compared to bagging, but it allows you to get reduce overfitting. However, it is important to understand that overfitting will still take place when training an arbitrarily large number of basic algorithms for a fixed $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NDHHPD13bLPt",
    "outputId": "3c06a1f2-3dfc-4c8b-de90-57dae18b2dd5"
   },
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=0.1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in enumerate(sizes):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQf3LBdTbLPu"
   },
   "source": [
    "Now let's test the effect described above on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYKKgEfObLPv",
    "outputId": "41d709d8-e8e8-4473-eefa-173ca6412e82"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = datasets.load_diabetes()\n",
    "X = ds.data\n",
    "Y = ds.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.5, test_size=0.5)\n",
    "\n",
    "MAX_ESTIMATORS = 300\n",
    "\n",
    "gbclf = BaggingRegressor(warm_start=True)\n",
    "err_train_bag = []\n",
    "err_test_bag = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_bag.append(1 - gbclf.score(X_train, Y_train))\n",
    "    err_test_bag.append(1 - gbclf.score(X_test, Y_test))\n",
    "    \n",
    "gbclf = GradientBoostingRegressor(warm_start=True, max_depth=2, learning_rate=0.1)\n",
    "err_train_gb = []\n",
    "err_test_gb = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_gb.append(1 - gbclf.score(X_train, Y_train))\n",
    "    err_test_gb.append(1 - gbclf.score(X_test, Y_test))\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(err_train_gb, label='GB')\n",
    "plt.plot(err_train_bag, label='Bagging')\n",
    "plt.legend()\n",
    "plt.title('Train')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(err_test_gb, label='GB')\n",
    "plt.plot(err_test_bag, label='Bagging')\n",
    "plt.legend()\n",
    "plt.title('Test')\n",
    "plt.gcf().set_size_inches(15,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO64OH4HbLPw"
   },
   "source": [
    "## Quality of algorithms\n",
    "\n",
    "Let's compare the behavior of two methods. For this purpose let's construct a composition of algorithms over decision trees on [Kaggle: Predicting a Biological Response](https://www.kaggle.com/c/bioresponse) task:\n",
    "\n",
    "The data is in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing a real biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are caclulated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "YElBCVCpc7df",
    "outputId": "a8b4c247-ba8e-4599-96dc-ad5fb3b8d101"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# https://drive.google.com/file/d/1yAOlsXMjSjI-XPwMGFq3PU6BKlib1xha\n",
    "data = pd.read_csv('train.csv')\n",
    "print(data.Activity.value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIkKid2QbLPw"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laPzA-bnfb0S",
    "outputId": "c5e2ba81-e476-4c62-cef4-6892da15a839"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AsW0wcwbLPx",
    "outputId": "8264af97-51a7-4eb3-cfb2-b615e00db421"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=250, learning_rate=0.2, verbose=True\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Evz-_IddbLPx",
    "outputId": "3df9f9db-080d-42f0-df13-6237b83a01e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for learning_rate in [1, 0.5, 0.3, 0.2, 0.1]:\n",
    "\n",
    "    gbm = GradientBoostingClassifier(\n",
    "        n_estimators=150, learning_rate=learning_rate, random_state=241\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    l = roc_auc_score\n",
    "\n",
    "    test_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_test)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        test_deviance[i] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    train_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_train)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        train_deviance[i] = roc_auc_score(y_train, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(test_deviance, 'r', linewidth=2)\n",
    "    plt.plot(train_deviance, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n",
    "    \n",
    "    plt.title('GBM eta=%.1f, test roc-auc=%.3f, best_est=%d' % (\n",
    "        learning_rate, test_deviance.max(), test_deviance.argmax()+1)\n",
    "             )\n",
    "    plt.xlabel('Number of trees')\n",
    "    plt.ylabel('Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aF6Td0SbLPx"
   },
   "source": [
    "In total, the best composition is built at $\\eta = 0.1$. It includes 24 basic algorithms, and reaches a value of 0.816 on the control sample. At the same time, a random forest with the same number of basic algorithms is inferior to gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbiO83dcbLPy",
    "outputId": "395961b2-d271-45cd-f05f-09a63a696291"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=24, random_state=0).fit(X_train, y_train)\n",
    "print ('Train RF ROC-AUC =', roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print ('Test RF ROC-AUC = ', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNMDYrE-bLPy"
   },
   "source": [
    "Note also that with all this, random forest, unlike gradient boosting, uses deep trees that require computing power to train them.\n",
    "\n",
    "To achieve the same quality, a random forest requires a much larger number of basic algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fP2RroTpbLPy",
    "outputId": "3c69dbae-f5ff-44a7-e362-3c71462080be"
   },
   "outputs": [],
   "source": [
    "for n_estimators in range(10, 101, 10):\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, n_jobs=4).fit(X_train, y_train\n",
    "                                                )\n",
    "    print (\n",
    "        n_estimators, 'trees: train ROC-AUC =',  \n",
    "        roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]), \n",
    "        'test ROC-AUC =',  \n",
    "        roc_auc_score(y_test, rf.predict_proba(X_test)[:,1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2kWFjo_bLPz"
   },
   "source": [
    "Finally, you can see [visualization](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) gradient boosting for decision trees of different depths for functions of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "A7ER_wJ2bLPz"
   },
   "source": [
    "## Modern gradient boosting libraries\n",
    "\n",
    "Previously, we used a naive version of gradient boosting from scikit-learn, [invented] (https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451) in 1999 by Friedman. Since then, many implementations have been proposed that prove to be better in practice. To date, three popular libraries that implement gradient boosting:\n",
    "* XGBoost. After its release, it quickly gained popularity and remained the standard until the end of 2016. The features of this library were discussed at the lecture.\n",
    "* LightGBM. A distinctive feature is the speed of composition construction. For example, the following trick is used to speed up learning: when building a tree vertex, instead of iterating over all the values of a feature, the values of the histogram of this feature are iterated over. So instead of $O (\\ell)$ requires $O$(#bins). In addition, unlike other libraries that build a tree by levels, LightGBM uses the best-first strategy, i.e., at each step, it builds the vertex that gives the greatest reduction in the functional. Thus, each tree is a chain with attached leaves, so the restriction on num_leaves is more meaningful.\n",
    "* CatBoost. Library from Yandex. Allows automatic processing of categorical features. In addition, the algorithm is less sensitive to the choice of specific hyperparameters. This reduces the time that a person spends on selecting the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWNyh3Z1bLPz"
   },
   "source": [
    "### Basic parameters\n",
    "\n",
    "(lightgbm/catboost)\n",
    "\n",
    "* objective-the functionality that the composition will be configured for\n",
    "* eta / learning_rate – learning rate\n",
    "* num_iterations / n_estimators – number of boosting iterations\n",
    "\n",
    "#### Parameters responsible for the complexity of trees\n",
    "* max_depth – maximum depth\n",
    "* max_leaves / num_leaves – the maximum number of vertices in the tree\n",
    "* gamma / min_gain_to_split-threshold for reducing the error function when splitting in the tree\n",
    "* min_data_in_leaf – minimum number of objects in the sheet\n",
    "* min_sum_hessian_in_leaf – the minimum sum of the weights of objects in the sheet, the minimum number of objects at which splitting is done\n",
    "* lambda-regularization coefficient (L2)\n",
    "* subsample / bagging_fraction – which part of the learning objects to use to build a single tree\n",
    "* colsample_bytree / feature_fraction – which part of the features to use to build a single tree\n",
    "\n",
    "The selection of all these parameters is a real art. But you can start setting them up with the most important parameters: learning_rate and n_estimators. Usually, one of them is fixed, and the remaining of these two parameters is selected (for example, n_estimators=1000 is fixed and learning_rate is selected). Next in importance is max_depth. Due to the fact that we are interested in shallow trees, it is usually sorted out from the range [3; 7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8dg1ou6bLP0",
    "outputId": "7b61a9ec-1eb6-4785-c273-a2dce8fbc7d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install catboost --quiet\n",
    "!pip install lightgbm --quiet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WARNING:</b> you might additionally need to install libomp on mac to avoid errors running lightgbm; easiest via brew: <br>\n",
    "`brew install libomp` <br>\n",
    "Check here for reference: <br>\n",
    "https://stackoverflow.com/questions/44937698/lightgbm-oserror-library-not-loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4RB6cnrbLP0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_surface(X, y, clf):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.8, random_state=241)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=241\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dc_2YKEbLP0"
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "zQXG-NUjbLP0",
    "outputId": "eef5bf89-f91b-4df1-c97d-5f97b28abfc7"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier \n",
    "\n",
    "clf = CatBoostClassifier(iterations=300, logging_level='Silent')\n",
    "clf.fit(X_train, y_train)\n",
    "plot_surface(X_test, y_test, clf)\n",
    "\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "1r4ymuWUbLP1",
    "outputId": "18cd8d4c-f3e1-4efd-8d56-997cd43aa985"
   },
   "outputs": [],
   "source": [
    "n_trees = [1, 5, 10, 100, 200, 300, 400, 500, 600, 700]\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for n in n_trees:\n",
    "    clf = CatBoostClassifier(iterations=n, logging_level='Silent')\n",
    "    clf.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_trees, quals_train, marker='.', label='train')\n",
    "plt.plot(n_trees, quals_test, marker='.', label='test')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzzB4U9nbLP1"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get error running cell below on mac, you might need to install libomp; easiest via brew: <br>\n",
    "`brew install libomp` <br>\n",
    "Check here for reference: <br>\n",
    "https://stackoverflow.com/questions/44937698/lightgbm-oserror-library-not-loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "XGxPT5jMbLP1",
    "outputId": "73ebb65b-80ca-44ef-e52c-60b0b050565f"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier(n_estimators=300, n_jobs=1) #n_jobs set to 1 to prevent\n",
    "                                                #hanging when using certain compilers\n",
    "clf.fit(X_train, y_train)\n",
    "plot_surface(X_test, y_test, clf)\n",
    "\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "BBpQlHDdbLP2",
    "outputId": "b2b74a4f-1ec1-4979-f7a6-88a62ddc7dad"
   },
   "outputs": [],
   "source": [
    "n_trees = [1, 5, 10, 100, 200, 300, 400, 500, 600, 700]\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for n in n_trees:\n",
    "    clf = LGBMClassifier(n_estimators=n, n_jobs=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_trees, quals_train, marker='.', label='train')\n",
    "plt.plot(n_trees, quals_test, marker='.', label='test')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZGWAiSybLP2"
   },
   "source": [
    "Now we will try to take a fixed number of trees, but we will change the depth of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "6rywLnoVbLP2",
    "outputId": "4c0128be-36f5-42d5-be13-4ba3f0a0b5bc"
   },
   "outputs": [],
   "source": [
    "depth = list(range(1, 30, 3))\n",
    "n_trees = 300\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for d in depth:\n",
    "    lgb = LGBMClassifier(n_estimators=n_trees, max_depth=d, n_jobs=1)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, lgb.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, lgb.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(depth, quals_train, marker='.', label='train')\n",
    "plt.plot(depth, quals_test, marker='.', label='test')\n",
    "plt.xlabel('Depth of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN7y5zeFbLP3"
   },
   "source": [
    "And compare it with Catboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "1PfQopL3bLP3",
    "outputId": "0ab8e6d7-d00a-4955-92b6-d31019a38527"
   },
   "outputs": [],
   "source": [
    "depth = list(range(1, 12, 2))\n",
    "n_trees = 300\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for d in depth:\n",
    "    clf = CatBoostClassifier(n_estimators=n_trees, max_depth=d, logging_level=\"Silent\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    q_train = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "    q_test = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    quals_train.append(q_train)\n",
    "    quals_test.append(q_test)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(depth, quals_train, marker='.', label='train')\n",
    "plt.plot(depth, quals_test, marker='.', label='test')\n",
    "plt.xlabel('Depth of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suH1dwh7bLP3"
   },
   "source": [
    "Now that we have great models, we need to save them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scRG1f_3bLP3"
   },
   "outputs": [],
   "source": [
    "lgb.booster_.save_model('lightgbm.txt')\n",
    "\n",
    "clf.save_model('catboost.cbm', format='cbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WeK3STrbLP4"
   },
   "source": [
    "And we'll upload them back when we need to apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qCbC_jibLP4"
   },
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(model_file='mode.txt')\n",
    "\n",
    "clf = clf.load_model('catboost.cbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3fo90TmbLP4"
   },
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vgaQGGAbLP4"
   },
   "source": [
    "As it was described in the lecture, blending is a \"meta-algorithm\", the prediction of which is built as a weighted sum of the basic algorithms. Consider a simple example of blending boosting and linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZrw0gdCbLP5"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    error = (y_true - y_pred) ** 2\n",
    "    return np.sqrt(np.mean(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Phqz-nKibLP5",
    "outputId": "93046ef5-eb3f-487f-c2e4-d2338be28cd9"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cbm = CatBoostRegressor(\n",
    "    iterations=100, max_depth=4, learning_rate=0.01, \n",
    "    loss_function='RMSE', logging_level='Silent'\n",
    ")\n",
    "cbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_cbm = cbm.predict(X_test)\n",
    "y_train_pred_cbm = cbm.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE GB = %.4f\" % rmse(y_train, y_train_pred_cbm))\n",
    "print(\"Test RMSE GB = %.4f\" % rmse(y_test, y_pred_cbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1AmqC8HcbLP5",
    "outputId": "1bda4c4b-9679-4858-f18b-f4667b761ff4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_train_pred_lr = lr.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train RMSE LR = %.4f\" % rmse(y_train, y_train_pred_lr))\n",
    "print(\"Test RMSE LR = %.4f\" % rmse(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GFVVlQcbLP5"
   },
   "source": [
    "For simplicity, we assume that the new algorithm $a(x)$ is represented as\n",
    "$$\n",
    "    a(x)\n",
    "    =\n",
    "    \\sum_{n = 1}^{N}\n",
    "    w_n b_n(x),\n",
    "$$\n",
    "where $\\sum_{n} w_n =1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbDiq9tIbLP6"
   },
   "outputs": [],
   "source": [
    "def select_weights(y_true, y_pred_1, y_pred_2):\n",
    "    grid = np.linspace(0, 1, 1000)\n",
    "    metric = []\n",
    "    for w_0 in grid:\n",
    "        w_1 = 1 - w_0\n",
    "        y_a = w_0 * y_pred_1 + w_1 * y_pred_2\n",
    "        metric.append([rmse(y_true, y_a), w_0, w_1])\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nKmQADdbLP6"
   },
   "outputs": [],
   "source": [
    "rmse_blending_train, w_0, w_1 = min(\n",
    "    select_weights(y_train, y_train_pred_cbm, y_train_pred_lr), \n",
    "    key=lambda x: x[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ch40D1MAbLP6",
    "outputId": "309b117f-9233-41f6-d26f-9b30788e0373"
   },
   "outputs": [],
   "source": [
    "rmse_blending_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90AwN4lmbLP6",
    "outputId": "daf1dca3-e882-423e-ac29-36d208cf398a"
   },
   "outputs": [],
   "source": [
    "rmse(y_test, y_pred_cbm * w_0 +  y_pred_lr * w_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9agPaLFMbLP7"
   },
   "source": [
    "As a result, we get a better quality on the test sample than each algorithm separately."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "C0jdQ3TFbLP8"
   ],
   "name": "sem09-gbm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
